{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The number in the brackets ([]), indicates the number of records to process. \n",
    "# Change the number for a larger sample size. \n",
    "title_dict = {}\n",
    "with open('history.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "    for entry in d[:500]:\n",
    "        if entry['title'] != '':\n",
    "            if entry['url'] not in title_dict:\n",
    "                title_dict[entry['url']] = entry['title']\n",
    "        else:\n",
    "            try:\n",
    "                r = urllib.urlopen(entry['url']).read()\n",
    "                soup = BeautifulSoup(r)\n",
    "                title_dict[entry['url']] = soup.title.string\n",
    "            except: \n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['Inbox - ce.sandoval09@gmail.com - Gmail', 'Download Anaconda Now! | Continuum', 'ACT Monday Workshop - ce.sandoval09@gmail.com - Gmail', 'Sent Mail - ce.sandoval09@gmail.com - Gmail', 'Drafts (57) - ce.sandoval09@gmail.com - Gmail', 'Messenger', 'Installing NLTK Data — NLTK 3.0 documentation', 'nltk download - Google Search', '20.6. urllib2 — extensible library for opening URLs — Python 2.7.13 documentation', 'Anaconda package list | Continuum Analytics: Documentation', 'Jupyter Notebook', 'parse_history.py (editing)', 'Dropbox (MIT)/Workshops_2/17.Spring ACT/', 'Dropbox (MIT)/Workshops_2/', 'Dropbox (MIT)/', 'Home', 'Messenger', 'Command not found: jupyter · Issue #2247 · jupyter/notebook', '3. Running the Jupyter Notebook — Jupyter/IPython Notebook Quick Start Guide 0.1 documentation', 'ipython - Unable to set up Jupyter Notebook - Stack Overflow', 'python - Jupyter notebook command does not work on Mac - Stack Overflow', '-bash: jupyter: command not found - Google Search', 'database - How to open this .DB file? - Stack Overflow', 'Confirmation of your Citi® Online activity - ce.sandoval09@gmail.com - Gmail', \"How do I print Safari's browsing history? | Official Apple Support Communities\", 'Where/How is Browser History Stored for Safari 8? | Official Apple Support Communities', 'BrowsingHistoryView 2.00 Free Download', 'Banking with Citi | Citi.com', 'Safari Browsing History (Mac) | Forensic Artifacts', 'How To: Export and save your browsing history', 'Sign Off Offers - Citibank', 'Account Information - Citibank', 'Payments - Citibank', 'Payments - Citibank', 'Banking with Citi | Citi.com', 'Download Complete | iExplorer | Macroplant', 'Export Safari Bookmarks & Browsing History from your iPhone or iPad | iExplorer | Macroplant', 'How to Export browsing history in Safari. | Official Apple Support Communities', 'export browsing history safari - Google Search', 'Backup Or Export Google Chrome History To CSV Excel, Text Or HTML', 'Can Chrome browser history be exported to an HTML file? - Super User', 'Engagement Window', 'Engagement Window', 'Account Information - Citibank', 'Contact Us- Banking and Personal Loans - Citibank', \"MIT Libraries' Catalog - Barton - Basic Search of Full Catalog\", 'Redirect remote sso - no id exist', 'Identification', 'PDS SSO', 'Citi® Credit Cards - Incorrect Information Entered', 'Compare Credit Card Offers & Apply Online - Citi.com', 'Banking with Citi | Citi.com', 'Contact Us- Credit Cards - Citibank', 'aa citi card contact number - Google Search', \"Students' Contact Information - ce.sandoval09@gmail.com - Gmail\", '\"MIT\" - ce.sandoval09@gmail.com - Gmail', '\"Berkeley\" (1) - ce.sandoval09@gmail.com - Gmail', 'Abhishek Agarwal', '(1) Carlos Emilio Sandoval Olascoaga', 'Carlos Emilio Sandoval Olascoaga', 'Larisa Ovalles Paulino added a new photo. - Larisa Ovalles Paulino', 'Facebook', 'Export History in Firefox? - Forums - CNET', 'History export & import - Mozilla Support Community', 'Places to CSV :: Add-ons for Firefox', 'history export firefox - Google Search', 'Norwell History Tools :: Add-ons for Firefox', 'export history :: Search :: Add-ons for Firefox', 'Form History Control :: Add-ons for Firefox', 'download history :: Search :: Add-ons for Firefox', 'history :: Search :: Add-ons for Firefox', 'Where to find and manage downloaded files in Firef... - Mozilla Support Community', 'Settings for privacy, browsing history and do-not-... - Mozilla Support Community', 'History Export Saves History As HTML, JSON, XML Or Text File [Firefox]', 'Addon to save browsing history firefox | Wilders Security Forums', 'How-To: Save and Restore Your Browsing History in Firefox', 'Settings for privacy, browsing history and do-not-... - Mozilla Support Community', 'save browsing history firefox - Google Search', 'Solved: Re: How to export browser history? - Mozilla Support Community', 'MZHistoryView: View the list of visited web sites in Firefox/Mozilla/Netscape browsers', 'Facebook', 'Solved: Re: How to export browser history? - Mozilla Support Community', 'how to backup, export & import browser history? - Mozilla Support Community', 'Export your browsing history before you delete it - gHacks Tech News', 'export browsing history firefox - Google Search', 'windows xp - How do I export the Browsing History in Firefox? - Super User', 'Export History - Chrome Web Store', 'export history firefox - Google Search', '(1) Haley Nesmith - Moments on walls, campgrounds and buses in Yosemite', 'Haley Nesmith - Moments on walls, campgrounds and buses in Yosemite', '(1) Haley Nesmith - Moments on walls, campgrounds and buses in Yosemite', 'Haley Nesmith - Moments on walls, campgrounds and buses in Yosemite', 'Haley Nesmith', '(1) Messenger', '(1) Rawr! - Carlos Emilio Sandoval Olascoaga', '(1) Rawr! - Carlos Emilio Sandoval Olascoaga', 'Haley Nesmith', '(1) Just kidding. Last (?) Ice climbing trip of the... - Carlos Emilio Sandoval Olascoaga', 'Just kidding. Last (?) Ice climbing trip of the... - Carlos Emilio Sandoval Olascoaga', 'pemn.github.io', 'WhatsApp Web', 'Messenger', 'Josh Smalley', 'Luc Deckinga', 'Gabriel Kaprielian', '(1) Last ice climbing trip of the season! - Carlos Emilio Sandoval Olascoaga', 'Last ice climbing trip of the season! - Carlos Emilio Sandoval Olascoaga', 'Kristen Henderson', 'David Migl', 'Redpoint Bouldering Gym Mexico - Posts', 'Google Calendar - Week of Apr 9, 2017', 'Google Calendar - Week of Apr 2, 2017', 'Meetin on Wednesday - ce.sandoval09@gmail.com - Gmail', 'Search results - ce.sandoval09@gmail.com - Gmail', '\"MIT\" - ce.sandoval09@gmail.com - Gmail', 'Inbox - ce.sandoval09@gmail.com - Gmail', 'Enjoy your Rock and Ice 240 Digital Edition! - ce.sandoval09@gmail.com - Gmail', 'decodes/LICENSE.txt at master · ksteinfe/decodes', 'ksteinfe/decodes: a platform agnostic generative design library for 3d designers', 'pemn/decodes: a platform agnostic generative design library for 3d designers', 'pemn (Paulo Ernesto)', 'decod-es', 'Network Graph · ksteinfe/decodes', 'Stargazers · ksteinfe/decodes', 'Watchers · ksteinfe/decodes', 'Notifications', 'Watchers · cesandoval/PaintingWithData_Riyadh', 'Stargazers · cesandoval/PaintingWithData_Riyadh', 'Kiel Moe | LinkedIn', 'Kiel Moe - Wikipedia', 'Kiel Moe | Harvard University Center for the Environment', 'Kiel Moe - Harvard Graduate School of Design', 'cesandoval/PaintingWithData_Riyadh: Riyadh Implementation of Painting with Data', 'Chase Bank - Credit Card, Mortgage, Auto, Banking Services', 'CREDIT CARD - chase.com', 'Chase Online', 'Thanks for choosing Bank of America. We look forward to serving you again soon.', 'Bank of America | Online Banking | SIGN OFF', 'Sign off Page', 'Bank of America | Online Banking | Accounts | Account Details | Account Activity', 'Bank of America | Online Banking | Accounts Overview', 'Bank of America — Banking, Credit Cards, Mortgages and Auto Loans', 'Thank you for purchasing tickets online! - ce.sandoval09@gmail.com - Gmail', 'Outlook', 'Outlook', 'Outlook', 'Continue', 'stream Me First and the Gimme Gimmes’ new greatest hits album, ‘Rake It In’', 'Pia Somar - H E E L S #girl #legs #me #tagsforlikes #esloquehay...', \"Stockholm truck attack suspect 'known to security services' | World news | The Guardian\", 'Stockholm Sweden - Facebook Search', 'Action sports, live events, and stories | Red Bull TV', 'Automattic/kue: Kue is a priority job queue backed by redis, built for node.js.', '(16) Dave Hause - Bury Me In Philly - YouTube', 'Amazon.com Sign In', 'Community Commons', 'Black Diamond Carabiner Recall - Black Diamond Warranty', 'Kevin Jorgeson & Ben Rueck Make First Free Ascent of the West Face of Sentinel', 'Connie Bee', 'Print Confirmation', 'Online Ticket Sales', 'Online Ticket Sales', 'Online Ticket Sales', 'Online Ticket Sales', 'Online Ticket Sales', 'Online Ticket Sales', 'Online Ticket Sales', 'Online Ticket Sales', 'Me First and the Gimme Gimmes tickets in Boston at Royale on Wed, Apr 19, 2017 - 8:00PM', 'Me First and the Gimme Gimmes - Boston Concert Tickets - Me First and the Gimme Gimmes Royale Tickets - April 19, 2017 | Bandsintown', 'On This Day', 'Me First And The Gimme Gimmes', 'Online Ticket Sales', 'Update Security Question Page', 'Online Ticket Sales', 'Online Ticket Sales', 'Online Ticket Sales', 'Me First And The Gimme Gimmes - Home', 'Lily Bui - That time I was in a documentary about New England...', 'clara hungr mit - Google Search', 'clara hungr - Google Search', 'Rideshare-Rumney 2017-04-09 - Google Sheets', 'Climbing at Rumney on Sunday April 9. - ce.sandoval09@gmail.com - Gmail', '42.3604333, -71.0978897 to 407 Washington St, Cambridge, MA 02139 - Google Maps', 'Google Maps', 'My location to 407 Washington St, Cambridge, MA 02139 - Google Maps', 'Google Maps', 'Google Maps', '407 Washington St, Cambridge, MA - Google Search', '407 Washington St - Google Search', 'MITOC Climbing Ride @ 0600 Tomorrow - ce.sandoval09@gmail.com - Gmail', 'Job Opportunities | Office of Sustainability', 'Job Opportunities | Office of Sustainability', 'Job Opportunities | Office of Sustainability', 'River Garden Documentary Screening - April 11 - ce.sandoval09@gmail.com - Gmail', 'Kue is super unreliable. Any alternatives for message queues that actually work? : node', 'https://hashnode.com/post/what-message-queue-is-best-fit-for-a-nodejs-application-ciibz8fjk01c8j3xteummv48w', 'node.js - Is there a compelling reason to use an AMQP based server over something like beanstalkd or redis? - Stack Overflow', 'Painting with Data', 'Painting with Data', 'Painting with Data', 'MIT School of Architecture + Planning - Posts', 'MIT Outing Club', \"Your TurboTax account: We've updated your info - ce.sandoval09@gmail.com - Gmail\", 'TurboTax® Tax Preparation Software, FREE Tax Filing, Efile Taxes, Income Tax Returns', 'TurboTax Online', 'My TurboTax®', 'My TurboTax® Login – Sign in to TurboTax to work on Your Tax Return', '617-253-4255 - Google Search', '617-253-4255 dial - Google Search', 'HR/Payroll Service Center | MIT VPF', 'mit payroll contact - Google Search', 'mit payroll - Google Search', 'Payroll | MIT VPF', 'Contact Us | MIT VPF', '986496045.pdf', 'Blue Cross Blue Shield of Massachusetts Message View', 'Blue Cross Blue Shield of Massachusetts Registration', 'BCBSMA Confirmation #1929106APR17 - ce.sandoval09@gmail.com - Gmail', \"Syria war: US launches missile strikes in response to 'chemical attack' - BBC News\", 'REMINDER! Dept. of Architecture CPW events/April 6 & 7 - ce.sandoval09@gmail.com - Gmail', 'MIT Climbing Wall', 'Painting with Data Ready for Use in Riyadh? - ce.sandoval09@gmail.com - Gmail', 'Your Repositories', 'cesandoval (Carlos Sandoval Olascoaga)', 'philipbelesky (Philip Belesky) / Repositories', 'philipbelesky (Philip Belesky) / Repositories', 'Your Followers', 'Painting with Data', 'Issues · cesandoval/PaintingWithData_Riyadh', 'Contributors to cesandoval/PaintingWithData_Riyadh', 'Commits · cesandoval/PaintingWithData_Riyadh', 'modified knn slider text · cesandoval/PaintingWithData_Riyadh@44ac87a', 'Rename Nearest Neighborhoods to something intuitive for novice user · Issue #60 · cesandoval/PaintingWithData_Riyadh', 'pcoords now supportsame property name, by adding the layername to the… · cesandoval/PaintingWithData_Riyadh@d2e9ffc', 'Data Explorer Aggregates Layers with Same Attribute Name · Issue #65 · cesandoval/PaintingWithData_Riyadh', 'Katia Sobolski - Blending in #working', '[ISO] Please Attend 4/13 Community Briefing on Immigration Laws and Policies - ce.sandoval09@gmail.com - Gmail', 'Katia Sobolski - Splash of color #architects', 'Forbes Welcome', 'Forbes Welcome', 'Forbes Welcome', 'deck.gl', 'deck.gl', 'deck.gl', 'image05.jpg (1500×952)', 'Visualize Data Sets on the Web with Uber Engineering’s deck.gl Framework - Uber Engineering Blog', 'Forbes Welcome', 'After Summiting Mt. Everest, He Returned Home to Face His Demons', 'Open Sourcing deck.gl 4.0: Uber Engineering’s Framework for Advanced Data Visualization - Uber Engineering Blog', 'Forbes Welcome', 'Fwd: Two proposals - ce.sandoval09@gmail.com - Gmail', 'Visit Autodesk FabLab? - ce.sandoval09@gmail.com - Gmail', 'Student Engagement | MIT CoLab', 'MIT CoLab Spring Newsletter - ce.sandoval09@gmail.com - Gmail', 'Update: Horowitz Award Check - ce.sandoval09@gmail.com - Gmail', 'Fwd: Two Pieces by Bjorn Sparrman at Yve YANG Gallery\\xa0 - ce.sandoval09@gmail.com - Gmail', 'Venmo | Patty Bolan', 'Patty Bolan wants to be friends with you on Venmo - ce.sandoval09@gmail.com - Gmail', 'Markdown Cheatsheet · adam-p/markdown-here Wiki', 'What is PATH? -- definition by The Linux Information Project (LINFO)', 'Gabriel Kaprielian', 'Gabriel Kaprielian', 'Faculty: Gabriel Kaprielian | Tyler School of Art', 'More developer guide in README.md · Issue #70 · cesandoval/PaintingWithData_Riyadh', 'gitignore - How do I ignore files in a directory in Git? - Stack Overflow', 'Ignoring files - User Documentation', 'Git - gitignore Documentation', 'Official Notice: Taxes due April 18. Finish today! - ce.sandoval09@gmail.com - Gmail', 'New open access policy for all MIT authors of scholarly articles - ce.sandoval09@gmail.com - Gmail', 'Lochie Ferrier | LinkedIn', '[eh-open] FW: Emailing Dorms for Mass CPR - ce.sandoval09@gmail.com - Gmail', 'Announcement! \"List Projects: Kenneth Tam\" opening at the List Center - ce.sandoval09@gmail.com - Gmail', 'Ice Climbing Season. Winter School. #MITOC #MIT - Carlos Emilio Sandoval Olascoaga', 'Warm ice. Ice climbing season. #MITOC - Carlos Emilio Sandoval Olascoaga', 'Chasing the last ice of the season. Mt.... - Carlos Emilio Sandoval Olascoaga', 'Aplicaciones SSO (2017-2018) - ce.sandoval09@gmail.com - Gmail', 'TurboTax® Tax Preparation Software, FREE Tax Filing, Efile Taxes, Income Tax Returns', 'Bank of America | Online Banking | Accounts | Account Details | Information and Services', 'Home', 'Find Username', 'Home', 'Register', 'Register', 'Register', 'Register', 'Home', 'Massachusetts Health Insurance | Blue Cross Blue Shield MA', '1099HC+2017+Sample+Form.pdf', 'Forms & Brochures', 'Forms & Brochures', 'Contact Us', 'Google Hangouts', '617-253-5979 dial - Google Search', '617-253-5979 - Google Search', 'Student Health Plans | MIT Medical', 'https://atlas.mit.edu/atlas/Main.action?tab=aboutMe&sub=w2', 'https://atlas.mit.edu/atlas/Main.action?tab=home&sub=fullCatalog', 'https://atlas.mit.edu/atlas/Main.action?tab=home', 'ATLAS - Navigating Your MIT World', 'MIT WebSIS: For Students', 'student_enroll.pdf', 'For Students | MIT Medical', 'Get 2017 health coverage. Health Insurance Marketplace | HealthCare.gov', 'What’s this new tax form? | MIT Medical', 'Are You Graduating? | MIT Medical', '1099-hc mit - Google Search'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = title_dict.values()\n",
    "\n",
    "clean_titles = []\n",
    "for ctitle in titles:\n",
    "    if ctitle != None:\n",
    "        clean_titles.append(ctitle)\n",
    "dictionary = corpora.Dictionary([word.lower().split(' ') for word in clean_titles])\n",
    "browsing_titles = [word.lower().split(' ') for word in clean_titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wordsEn.txt\") as word_file:\n",
    "    english_words = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "def is_english_word(word):\n",
    "    return word.lower() in english_words\n",
    "\n",
    "only_english_ids = [word[1] for word in dictionary.token2id.items() if not is_english_word(word[0])]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get a list of stop words from the nltk library\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "# DICTERATOR: remove stop words and words that appear only once \n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "\n",
    "# filter the tokens from the corpora dict\n",
    "dictionary.filter_tokens(stop_ids + once_ids + only_english_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1274ca3c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify() \n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a dictionary and a list of ids, get the words that correspond to the ids\n",
    "def get_singles(dictionary, ids):\n",
    "    for word_id in ids:\n",
    "        yield dictionary.get(word_id)\n",
    "\n",
    "# eliminate the words that appear only once        \n",
    "def filter_singles(singles, texts):\n",
    "    for text in texts:\n",
    "        new_list = []\n",
    "        for word in text:\n",
    "            if word not in singles:\n",
    "                new_list.append(word)\n",
    "        yield new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "singles = get_singles(dictionary, once_ids)\n",
    "filtered_texts = filter_singles(singles, [word.lower().split(' ') for word in clean_titles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Bag of words\n",
    "mm = [dictionary.doc2bow(text) for text in filtered_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of topics for the classification\n",
    "num_topics = 2\n",
    "\n",
    "# Trains the LDA models with the corpus and dictionary previously created\n",
    "lda = models.ldamodel.LdaModel(corpus=list(mm), id2word=dictionary, num_topics=num_topics, \n",
    "                               update_every=1, chunksize=10000, passes=10, iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.076*\"history\" + 0.055*\"search\" + 0.049*\"export\" + 0.038*\"browsing\" + 0.034*\"google\" + 0.025*\"climbing\" + 0.024*\"support\" + 0.022*\"ice\" + 0.022*\"community\" + 0.017*\"home\" + 0.015*\"last\" + 0.015*\"welcome\" + 0.014*\"download\" + 0.014*\"sign\" + 0.014*\"new\" + 0.013*\"browser\" + 0.012*\"campgrounds\" + 0.012*\"moments\" + 0.012*\"buses\" + 0.012*\"yosemite\" + 0.012*\"trip\" + 0.012*\"job\" + 0.012*\"save\" + 0.012*\"messenger\" + 0.012*\"safari\"'),\n",
       " (1,\n",
       "  '0.048*\"google\" + 0.034*\"sales\" + 0.034*\"ticket\" + 0.029*\"data\" + 0.023*\"tax\" + 0.022*\"banking\" + 0.020*\"contact\" + 0.020*\"first\" + 0.018*\"blue\" + 0.018*\"information\" + 0.018*\"painting\" + 0.018*\"credit\" + 0.017*\"bank\" + 0.016*\"search\" + 0.015*\"maps\" + 0.015*\"account\" + 0.014*\"stack\" + 0.014*\"overflow\" + 0.013*\"america\" + 0.012*\"health\" + 0.012*\"tickets\" + 0.012*\"washington\" + 0.012*\"april\" + 0.012*\"register\" + 0.012*\"free\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints all groups and their main words\n",
    "lda.print_topics(num_topics=num_topics, num_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of processed topics obtained by training an LDA model, and return them as individual lists of topics and frequencies\n",
    "def parse_topics(filepath):\n",
    "    with open(filepath, 'rU') as f:\n",
    "        reader = list(csv.reader(f))\n",
    "        header = reader[0]\n",
    "        reader.pop(0)\n",
    "        topics = []\n",
    "        freqs = []\n",
    "        for row in reader:\n",
    "            freq = []\n",
    "            topic = []\n",
    "            row.pop(0)\n",
    "            for ind, element in enumerate(row):\n",
    "                if ind%2 == 0:\n",
    "                    try: \n",
    "                        fr = row[ind+1]\n",
    "                    except: \n",
    "                        fr = ''\n",
    "                    if fr != '':\n",
    "                        topic.append(element)\n",
    "                        freq.append(row[ind+1])\n",
    "            topics.append(topic)\n",
    "            freqs.append(freq)\n",
    "        return topics, freqs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "([['Travel', 'Mail'], ['Chemistry', 'Economics']],\n",
    " [['0.5', '0.25'], ['0.7', '0.8']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns the topics to the documents in corpus\n",
    "lda_corpus = lda[mm]\n",
    "threshold = 1/float(num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/csandova/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import requests\n",
    "from knowledge_ontologies.spellcheck import spellcheck\n",
    "import re\n",
    "import json\n",
    "from pprint import pprint\n",
    "from knowledge_ontologies.Scraper import *\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "import spacy\n",
    "import itertools\n",
    "import numpy as np\n",
    "from knowledge_ontologies.check_lemmas import check_lemmas\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from knowledge_ontologies.config import L1,L2,L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ontology=L1+L2+L3\n",
    "full_list=set(full_ontology)\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_closest_topics(user_keyword):\n",
    "    with open ('knowledge_ontologies/vectors_map.json','r') as f:\n",
    "        vector_dict=json.load(f)\n",
    "\n",
    "        keyword=None\n",
    "        while keyword!= 'N':\n",
    "            if keyword== 'N':\n",
    "                break\n",
    "\n",
    "            keyword=user_keyword.lower()\n",
    "\n",
    "            checked=spellcheck(keyword)\n",
    "\n",
    "            ##check if it is an existing key in the ontology\n",
    "            if keyword in full_list:\n",
    "                print(keyword,11111111111)\n",
    "                best = keyword\n",
    "                return best\n",
    "            else:\n",
    "                secondary_check=check_lemmas(checked)\n",
    "                if secondary_check:\n",
    "                    print(secondary_check,2222222222222222)\n",
    "#                     continue\n",
    "\n",
    "            keyword_doc = list(nlp.pipe(checked,\n",
    "              batch_size=10000,\n",
    "              n_threads=1))\n",
    "\n",
    "\n",
    "            if keyword_doc[0].has_vector:\n",
    "                keyword_vector=np.array([keyword_doc[0].vector])\n",
    "            else:\n",
    "                spacy.vocab[0].vector\n",
    "\n",
    "            intermidiate_results=[]\n",
    "            best=None\n",
    "            best_similarity=0\n",
    "            ## First level\n",
    "\n",
    "            l1_keys=treeL1.keys()\n",
    "            arrays=[]\n",
    "            order=[]\n",
    "            for k in l1_keys:\n",
    "                order.append(k)\n",
    "                arrays.append(np.array(vector_dict[k]))\n",
    "\n",
    "            simple_sim = cosine_similarity(keyword_vector, arrays)\n",
    "            topic_idx = simple_sim.argmax(axis=1)[0]\n",
    "            best_similarity=np.amax(simple_sim)\n",
    "            result=order[topic_idx]\n",
    "            best=result\n",
    "            # print('r1: ', result )\n",
    "\n",
    "            ##second LEvel\n",
    "            l2_keys=[word for word in treeL1[result] if word in treeL2]\n",
    "            # print('choices: ', l2_keys)\n",
    "            arrays=[]\n",
    "            order=[]\n",
    "            for k in l2_keys:\n",
    "                order.append(k)\n",
    "                arrays.append(np.array(vector_dict[k]))\n",
    "\n",
    "            simple_sim = cosine_similarity(keyword_vector, arrays)\n",
    "            topic_idx = simple_sim.argmax(axis=1)[0]\n",
    "            result=order[topic_idx]\n",
    "            maxv=np.amax(simple_sim)\n",
    "            if (maxv>=best_similarity):\n",
    "                best_similarity=maxv\n",
    "                best=result\n",
    "            # print('r2: ',result)\n",
    "\n",
    "\n",
    "            options=[ word for word in treeL2[result] if word in full_ontology]\n",
    "            # print('choices: ', options)\n",
    "            #print('options',options)\n",
    "            arrays=[]\n",
    "            for k in options:\n",
    "                #print(k,len(vector_dict))\n",
    "                arrays.append(np.array(vector_dict[k]))\n",
    "\n",
    "            simple_sim = cosine_similarity(keyword_vector, arrays)\n",
    "            topic_idx = simple_sim.argmax(axis=1)[0]\n",
    "            maxv=np.amax(simple_sim)\n",
    "            result=options[topic_idx]\n",
    "            if (maxv>=best_similarity):\n",
    "                best_similarity=maxv\n",
    "                best=result\n",
    "            print( 'classified as: '+best)\n",
    "            return best\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usertopics_to_ontologies(all_topics):\n",
    "    all_ontologies = []\n",
    "    print(all_topics)\n",
    "    for topics in all_topics:\n",
    "        ontologies = []\n",
    "        for topic in topics:\n",
    "            ontologies.append(classify_closest_topics(topic))\n",
    "        all_ontologies.append(ontologies)\n",
    "    print (all_ontologies)\n",
    "    return all_ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csandova/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: 'U' mode is deprecated\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([['science', 'economics'], ['asfasf', 'afsafsa']],\n",
       " [['0.5', '0.25'], ['0.7', '0.8']])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics, frequencies = parse_topics('knowledge_topic_classification.csv')\n",
    "topics, frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['science', 'economics'], ['asfasf', 'afsafsa']]\n",
      "food preparation 2222222222222222\n",
      "classified as: leisure\n",
      "economics 11111111111\n",
      "classified as: community\n",
      "classified as: community\n",
      "[['leisure', 'economics'], ['community', 'community']]\n"
     ]
    }
   ],
   "source": [
    "topics = usertopics_to_ontologies(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a corpus trained with the LDA classifier, and a threshold, classify the browsing history into the groups \n",
    "def classify(lda_corpus, texts, cluster_num, threshold, words=None, frequencies=None):\n",
    "    for i,j in zip(lda_corpus, texts):\n",
    "        try: \n",
    "            if i[cluster_num][1] > threshold :\n",
    "                classified_list = [j, words[cluster_num], frequencies[cluster_num]]\n",
    "                yield classified_list\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes the topic classification of a given topic, and other data of the topics and writes a new json to be spatially joined\n",
    "def topic_to_json(topic_num, topics, frequencies):\n",
    "    for i, record in enumerate(classify(lda_corpus, clean_titles, topic_num, threshold, topics, frequencies)):  \n",
    "        title, topic, frequency = record\n",
    "        with open('topics/%stopic_history.json' %(str(topic_num)+'_'+str(i)), 'w') as f:\n",
    "            f.write( json.dumps({'id': str(topic_num)+'_'+str(i), 'title':title, 'topic':topic, 'frequency':frequency}))\n",
    "            #print 'wrote tweet %s' %(tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every topic group, write json files for every tweet\n",
    "for topic_num in np.arange(num_topics):#lda_corpus, jsons_to_mm_tuple(twi_path), topic_num, threshold, num_topics): \n",
    "    topic_to_json(topic_num, topics, frequencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'community': 433,\n",
       " 'leisure': 203,\n",
       " 'economics': 203,\n",
       " 'travel': 4,\n",
       " 'places to eat and drink': 4}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "onlyfiles = [f for f in listdir('topics') if isfile(join('topics', f))]\n",
    "count_topics = {}\n",
    "for file in onlyfiles:\n",
    "    if file.endswith('.json'):\n",
    "        with open('topics/'+file, 'r') as f:\n",
    "            curr_record = json.load(f)\n",
    "            curr_topics = curr_record['topic']\n",
    "            for curr_topic in curr_topics:\n",
    "                if curr_topic not in count_topics:\n",
    "                    count_topics[curr_topic] = 0\n",
    "                else: \n",
    "                    count_topics[curr_topic] += 1\n",
    "\n",
    "\n",
    "count_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_vals = count_topics.keys()\n",
    "y_vals = count_topics.values()\n",
    "\n",
    "\n",
    "y_pos = np.arange(len(x_vals))\n",
    " \n",
    "plt.bar(y_pos, y_vals, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, x_vals)\n",
    "plt.ylabel('Number of Ocurrences')\n",
    "plt.title('Domains of Knowledge')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2020-01-31T01:59:40+0000 [stdout#info] Dictionary(0 unique tokens: [])\n",
    "2020-01-31T01:59:40+0000 [twisted.web.wsgi._ErrorStream#error] [2020-01-31 01:59:40,207] ERROR in app: Exception on /analyzer/titles/bbKaPJCMN9WM9LQ4VNNQ [POST]\n",
    "\tTraceback (most recent call last):\n",
    "\t  File \"/home/ubuntu/backend/backend/env/lib/python3.5/site-packages/flask/app.py\", line 2446, in wsgi_app\n",
    "\t    response = self.full_dispatch_request()\n",
    "\t  File \"/home/ubuntu/backend/backend/env/lib/python3.5/site-packages/flask/app.py\", line 1951, in full_dispatch_request\n",
    "\t    rv = self.handle_user_exception(e)\n",
    "\t  File \"/home/ubuntu/backend/backend/env/lib/python3.5/site-packages/flask/app.py\", line 1820, in handle_user_exception\n",
    "\t    reraise(exc_type, exc_value, tb)\n",
    "\t  File \"/home/ubuntu/backend/backend/env/lib/python3.5/site-packages/flask/_compat.py\", line 39, in reraise\n",
    "\t    raise value\n",
    "\t  File \"/home/ubuntu/backend/backend/env/lib/python3.5/site-packages/flask/app.py\", line 1949, in full_dispatch_request\n",
    "\t    rv = self.dispatch_request()\n",
    "\t  File \"/home/ubuntu/backend/backend/env/lib/python3.5/site-packages/flask/app.py\", line 1935, in dispatch_request\n",
    "\t    return self.view_functions[rule.endpoint](**req.view_args)\n",
    "\t  File \"/home/ubuntu/backend/backend/backend/routes/analyzer.py\", line 11, in handle_titles\n",
    "\t    results = classifier.process_titles(request.json)\n",
    "\t  File \"/home/ubuntu/backend/backend/backend/ml/classifier.py\", line 80, in process_titles\n",
    "\t    update_every=1, chunksize=10000, passes=10, iterations=50)\n",
    "\t  File \"/home/ubuntu/backend/backend/env/lib/python3.5/site-packages/gensim/models/ldamodel.py\", line 441, in __init__\n",
    "\t    raise ValueError(\"cannot compute LDA over an empty collection (no terms)\")\n",
    "\tValueError: cannot compute LDA over an empty collection (no terms)\n",
    "2020-01-31T01:59:40+0000 [twisted.web.wsgi._ErrorStream#error] "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2020-02-03T13:08:16+0000 [stdout#info] Dictionary(37 unique tokens: ['icons', 'dismiss', 'engineers', 'first', 'regular']...)\n",
    "2020-02-03T13:08:17+0000 [stdout#info] dismiss\n",
    "2020-02-03T13:08:17+0000 [stdout#info] classified as: shipping\n",
    "2020-02-03T13:08:17+0000 [stdout#info] awesome\n",
    "2020-02-03T13:08:17+0000 [stdout#info] Inconclusive inputted text,awesome could be a variety of words\n",
    "2020-02-03T13:08:17+0000 [stdout#info] [('awesome', 0.0)]\n",
    "2020-02-03T13:08:17+0000 [stdout#info] classified as: community\n",
    "2020-02-03T13:08:17+0000 [stdout#info] news\n",
    "2020-02-03T13:08:18+0000 [stdout#info] personal health data 2222222222222222\n",
    "2020-02-03T13:08:18+0000 [stdout#info] classified as: places to eat and drink\n",
    "2020-02-03T13:08:18+0000 [stdout#info] time\n",
    "2020-02-03T13:08:18+0000 [stdout#info] classified as: places to eat and drink\n",
    "2020-02-03T13:08:18+0000 [stdout#info] act\n",
    "2020-02-03T13:08:18+0000 [stdout#info] transport 2222222222222222\n",
    "2020-02-03T13:08:18+0000 [stdout#info] classified as: community\n",
    "2020-02-03T13:08:18+0000 [stdout#info] app\n",
    "2020-02-03T13:08:18+0000 [stdout#info] Inconclusive inputted text,app could be a variety of words\n",
    "2020-02-03T13:08:18+0000 [stdout#info] [('pp', 0.8579387186629527), ('apt', 0.09749303621169916), ('rapp', 0.027855153203342618), ('ape', 0.011142061281337047), ('asp', 0.002785515320334262), ('amp', 0.002785515320334262)]\n",
    "2020-02-03T13:08:18+0000 [twisted.web.wsgi._ErrorStream#error] [2020-02-03 13:08:18,304] ERROR in app: Exception on /analyzer/keywords/PzeKQ1LafElI7DFmnOEW [POST]\n",
    "        Traceback (most recent call last):\n",
    "          File \"/home/ubuntu/backend/backend/env/lib/python3.5/site-packages/flask/app.py\", line 2446, in wsgi_app\n",
    "            response = self.full_dispatch_request()\n",
    "          File \"/home/ubuntu/backend/backend/env/lib/python3.5/site-packages/flask/app.py\", line 1951, in full_dispatch_request\n",
    "            rv = self.handle_user_exception(e)\n",
    "          File \"/home/ubuntu/backend/backend/env/lib/python3.5/site-packages/flask/app.py\", line 1820, in handle_user_exception\n",
    "            reraise(exc_type, exc_value, tb)\n",
    "          File \"/home/ubuntu/backend/backend/env/lib/python3.5/site-packages/flask/_compat.py\", line 39, in reraise\n",
    "            raise value\n",
    "          File \"/home/ubuntu/backend/backend/env/lib/python3.5/site-packages/flask/app.py\", line 1949, in full_dispatch_request\n",
    "            rv = self.dispatch_request()\n",
    "          File \"/home/ubuntu/backend/backend/env/lib/python3.5/site-packages/flask/app.py\", line 1935, in dispatch_request\n",
    "            return self.view_functions[rule.endpoint](**req.view_args)\n",
    "          File \"/home/ubuntu/backend/backend/backend/routes/analyzer.py\", line 27, in handle_keywords\n",
    "            results = classifier.count_topics(topics, model['model'], model['titles'], model['corpus'])\n",
    "          File \"/home/ubuntu/backend/backend/backend/ml/classifier.py\", line 213, in count_topics\n",
    "            ontologies = usertopics_to_ontologies(topics)\n",
    "          File \"/home/ubuntu/backend/backend/backend/ml/classifier.py\", line 206, in usertopics_to_ontologies\n",
    "            ontologies.append(classify_closest_topics(topic))\n",
    "          File \"/home/ubuntu/backend/backend/backend/ml/classifier.py\", line 133, in classify_closest_topics\n",
    "            secondary_check = check_lemmas(checked)\n",
    "          File \"/home/ubuntu/backend/backend/backend/ml/knowledge_ontologies/check_lemmas.py\", line 30, in check_lemmas\n",
    "            lemma_syns=wn.synsets(input_lemma)[0]\n",
    "        IndexError: list index out of range\n",
    "2020-02-03T13:08:18+0000 [twisted.web.wsgi._ErrorStream#error]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
